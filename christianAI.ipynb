{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ef2597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/eallen/.local/lib/python3.13/site-packages (2.9.0.dev20250824+cu129)\n",
      "Requirement already satisfied: transformers in /home/eallen/.local/lib/python3.13/site-packages (4.55.4)\n",
      "Requirement already satisfied: accelerate in /home/eallen/.local/lib/python3.13/site-packages (1.10.1)\n",
      "Requirement already satisfied: sentence-transformers in /home/eallen/.local/lib/python3.13/site-packages (5.1.0)\n",
      "Requirement already satisfied: faiss-cpu in /home/eallen/.local/lib/python3.13/site-packages (1.12.0)\n",
      "Requirement already satisfied: pandas in /home/eallen/.local/lib/python3.13/site-packages (2.3.2)\n",
      "Requirement already satisfied: datasets in /home/eallen/.local/lib/python3.13/site-packages (4.0.0)\n",
      "Requirement already satisfied: peft in /home/eallen/.local/lib/python3.13/site-packages (0.17.1)\n",
      "Requirement already satisfied: trl in /home/eallen/.local/lib/python3.13/site-packages (0.21.0)\n",
      "Requirement already satisfied: bitsandbytes in /home/eallen/.local/lib/python3.13/site-packages (0.47.0)\n",
      "Requirement already satisfied: filelock in /home/eallen/.local/lib/python3.13/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.13/site-packages (from torch) (74.1.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/eallen/.local/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/eallen/.local/lib/python3.13/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/eallen/.local/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/eallen/.local/lib/python3.13/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.9.86 in /home/eallen/.local/lib/python3.13/site-packages (from torch) (12.9.86)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.9.79 in /home/eallen/.local/lib/python3.13/site-packages (from torch) (12.9.79)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.9.79 in /home/eallen/.local/lib/python3.13/site-packages (from torch) (12.9.79)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/eallen/.local/lib/python3.13/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.9.1.4 in /home/eallen/.local/lib/python3.13/site-packages (from torch) (12.9.1.4)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.4.1.4 in /home/eallen/.local/lib/python3.13/site-packages (from torch) (11.4.1.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.10.19 in /home/eallen/.local/lib/python3.13/site-packages (from torch) (10.3.10.19)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.5.82 in /home/eallen/.local/lib/python3.13/site-packages (from torch) (11.7.5.82)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.10.65 in /home/eallen/.local/lib/python3.13/site-packages (from torch) (12.5.10.65)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/eallen/.local/lib/python3.13/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/eallen/.local/lib/python3.13/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/eallen/.local/lib/python3.13/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.9.79 in /home/eallen/.local/lib/python3.13/site-packages (from torch) (12.9.79)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.9.86 in /home/eallen/.local/lib/python3.13/site-packages (from torch) (12.9.86)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.14.1.1 in /home/eallen/.local/lib/python3.13/site-packages (from torch) (1.14.1.1)\n",
      "Requirement already satisfied: pytorch-triton==3.4.0+gitf7888497 in /home/eallen/.local/lib/python3.13/site-packages (from torch) (3.4.0+gitf7888497)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/eallen/.local/lib/python3.13/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib64/python3.13/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib64/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/lib64/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/eallen/.local/lib/python3.13/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/eallen/.local/lib/python3.13/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/lib64/python3.13/site-packages (from accelerate) (6.1.1)\n",
      "Requirement already satisfied: scikit-learn in /home/eallen/.local/lib/python3.13/site-packages (from sentence-transformers) (1.7.1)\n",
      "Requirement already satisfied: scipy in /home/eallen/.local/lib/python3.13/site-packages (from sentence-transformers) (1.16.1)\n",
      "Requirement already satisfied: Pillow in /usr/lib64/python3.13/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/lib/python3.13/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/eallen/.local/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/eallen/.local/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/eallen/.local/lib/python3.13/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/eallen/.local/lib/python3.13/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /home/eallen/.local/lib/python3.13/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/eallen/.local/lib/python3.13/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/eallen/.local/lib/python3.13/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/eallen/.local/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3.13/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/eallen/.local/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/eallen/.local/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/eallen/.local/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/eallen/.local/lib/python3.13/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/eallen/.local/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/eallen/.local/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/eallen/.local/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/eallen/.local/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/eallen/.local/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/eallen/.local/lib/python3.13/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers accelerate sentence-transformers faiss-cpu pandas datasets peft trl bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b68f7e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50936aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/kjv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c3f36fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "themes = {\n",
    "    \"love\": [\"love\", \"charity\"],\n",
    "    \"faith\": [\"faith\", \"believe\", \"trust\"],\n",
    "    \"sin\": [\"sin\", \"iniquity\", \"transgression\", \"forbidden\"],\n",
    "    \"creation\": [\"create\", \"made\", \"beginning\"],\n",
    "    \"wisdom\": [\"wisdom\", \"understanding\", \"knowledge\"],\n",
    "    \"forgiveness\": [\"forgive\", \"forgiveness\", \"pardon\", \"mercy\"],\n",
    "    \"prayer\": [\"pray\", \"prayer\", \"ask\", \"supplication\"],\n",
    "    \"hope\": [\"hope\", \"promise\", \"wait\", \"salvation\"],\n",
    "    \"justice\": [\"justice\", \"correct\", \"righteous\"],\n",
    "    \"unity\": [\"unity\", \"community\", \"choosen\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aee2024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_theme_examples(theme, keywords, max_verses=3):\n",
    "    matches = df[df[\"Text\"].str.contains(\"|\".join(keywords), case=False)]\n",
    "    if len(matches) == 0:\n",
    "        return None\n",
    "    matches = matches.sample(min(max_verses, len(matches)))  # random sample\n",
    "    verses = [f\"{row['Book Name']} {row['Chapter']}:{row['Verse']} - {row['Text']}\"\n",
    "              for _, row in matches.iterrows()]\n",
    "    question = f\"What does the Bible say about {theme}?\"\n",
    "    answer = \" \".join(verses)\n",
    "    return {\"prompt\": question, \"completion\": answer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd606501",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_thematic = []\n",
    "for theme, keywords in themes.items():\n",
    "    for _ in range(100):  # 30 examples per theme\n",
    "        example = collect_theme_examples(theme, keywords)\n",
    "        if example:\n",
    "            qa_thematic.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd29045",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_lookup = [\n",
    "    {\n",
    "        \"prompt\": f\"What does {row['Book Name']} {row['Chapter']}:{row['Verse']} say?\",\n",
    "        \"completion\": row[\"Text\"]\n",
    "    }\n",
    "    for _, row in df.sample(15000, random_state=42).iterrows()\n",
    "]\n",
    "\n",
    "qa_all = qa_lookup + qa_thematic\n",
    "random.shuffle(qa_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eae4787a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14400/14400 [00:00<00:00, 17564.46 examples/s]\n",
      "Map: 100%|██████████| 1600/1600 [00:00<00:00, 17919.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_list(qa_all)\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "def format_example(example):\n",
    "    return {\n",
    "        \"text\": f\"### Question:\\n{example['prompt']}\\n\\n### Answer (in KJV style):\\n{example['completion']}\"\n",
    "    }\n",
    "\n",
    "train_dataset = dataset[\"train\"].map(format_example)\n",
    "eval_dataset = dataset[\"test\"].map(format_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dcc4b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.float16\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config = bnb_config,\n",
    "    offload_folder=\"offload\",\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "523087da",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c32da8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "model.enable_input_require_grads()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ff9b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora_\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e1d51fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding EOS to train dataset: 100%|██████████| 5000/5000 [00:00<00:00, 16453.17 examples/s]\n",
      "Tokenizing train dataset:   0%|          | 0/5000 [00:00<?, ? examples/s]/home/eallen/.local/lib/python3.13/site-packages/trl/trainer/sft_trainer.py:767: UserWarning: Mismatch between tokenized prompt and the start of tokenized prompt+completion. This may be due to unexpected tokenizer behavior, whitespace issues, or special token handling. Verify that the tokenizer is processing text consistently.\n",
      "  warnings.warn(\n",
      "Tokenizing train dataset: 100%|██████████| 5000/5000 [00:01<00:00, 4163.43 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 5000/5000 [00:00<00:00, 988942.75 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 500/500 [00:00<00:00, 15900.40 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 500/500 [00:00<00:00, 4013.47 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 500/500 [00:00<00:00, 242810.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./christAIn\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset.shuffle().select(range(5000)),\n",
    "    eval_dataset=eval_dataset.shuffle().select(range(500)),\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "952241df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight']\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = \"lora_\" in name\n",
    "\n",
    "# quick check\n",
    "trainable = [n for n, p in model.named_parameters() if p.requires_grad]\n",
    "print(\"Trainable parameters:\", trainable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5013d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 07:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.397400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.262600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.343500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.195900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.256700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.281600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.084200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.104100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.926800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>3.020900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.960800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.851400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.989100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.923500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.951000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.940100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.838100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.904900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.852700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.896900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.758600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.685500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.918400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.782100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.878300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.977500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.830100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.993400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.840800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.844000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.785500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.847000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.783400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.859000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.833000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.696600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>2.854500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.682800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.693800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.725700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.737900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.722800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>2.748600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.780400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.831500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.707300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>2.723100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.800400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>2.757200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.822900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.718200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.737900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>2.823200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.845100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>2.708100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.652600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>2.735200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.743900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>2.741600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.723000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.692000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.716900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>2.706900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.739000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>2.644000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.589100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>2.630100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.833200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>2.809200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.682600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.722200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>2.776900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>2.686200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>2.677500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>2.697300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.615400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>2.565500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>2.709800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>2.685800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>2.562300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.669200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>2.745100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>2.745100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>2.788800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>2.755500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.626800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>2.737400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>2.727600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>2.726400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>2.556200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.674500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>2.727500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>2.770900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>2.807000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>2.735300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.766100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>2.731400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>2.730700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>2.720300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>2.586500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.656900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>2.752000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>2.697200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>2.668400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>2.714600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.604400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>2.723500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>2.781000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>2.817200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>2.708100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>2.737600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>2.668100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>2.739900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>2.657000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>2.628700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.667400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>2.717500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>2.630700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>2.602000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>2.767300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.663600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>2.456700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>2.720900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>2.683600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>2.605200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.694900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>2.611700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>2.697100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>2.609500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>2.688400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.665500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>2.749900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>2.783800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>2.599800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>2.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.731700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>2.603300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>2.617700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>2.763200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>2.644800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>2.737800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>2.695900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>2.696300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>2.664300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>2.730200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.783400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>2.739000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>2.807100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>2.610700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>2.614700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>2.617800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>2.600300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>2.611800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>2.707500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>2.648800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.623000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>2.751100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>2.700100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>2.597100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>2.609300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>2.730700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>2.681100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>2.635600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>2.675100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>2.622800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>2.695800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>2.653400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>2.681000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>2.780600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>2.761400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>2.654200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>2.748800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>2.691800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>2.700900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.628900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>2.630500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>2.693500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>2.672400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>2.608600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>2.676400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>2.669300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>2.786800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "trainer.save_model(\"./christAIn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92395559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question:\n",
      "What does the Bible say about love?\n",
      "\n",
      "### Answer (in KJV style):\n",
      "\n",
      "**Proverbs 12:8** And when a man loveth, he is not as one that liveth; for his heart is more than flesh. **Proverbs 13:5** And the way of the wicked is to destroy the poor in their time, and to destroy the widow's daughter; but the way of the righteous shall be good, and the ways of the upright shall be blessed. **Proverbs 14:7** The love of money is a root of all evil. **Proverbs 16:29** The wicked will do great wrong, and the wicked will cause great damage. **Proverbs 17:16** Love is the greatest pleasure, and the best thing that ever was seen. **Proverbs 19:10** A friend without a friend is a stranger, and a friend without a friend is a stranger indeed. **Proverbs 24:10** The wise are full of knowledge, and the prudent have understanding. **Proverbs 26:12** Let not your love be so heavy as to make you sad, nor let it be so light as to make you angry. **Proverbs 27:11** He that loves others will love himself also. **Proverbs 29:15** If there be any affection in a man, it is because he loves another; if there be no affection in him, it is because he hates another. **Proverbs 30:30** And a father who loves his son will guard him from death; and a father who loves his son will save him from destruction. **Proverbs 31:13** For a father's care is better than gold, and his son's wisdom than jewels set in stone. **Proverbs 32:32** A father that will not provide for his children, will not provide for himself either. **Proverbs 33:24** But a father will give his son to the care of his sons, and his daughters to the care of his daughters. **Proverbs 33:25** A father will teach his children wisdom, and righteousness, and judgment, and faithfulness; and he will put them in the right path. **Proverbs 33:26** For a father will give his children a good reward, and a mother an inheritance. **Proverbs\n",
      "### Question:\n",
      "What was the original sin?\n",
      "\n",
      "### Answer (in KJV style):\n",
      "\n",
      "And there was once a man of old, in the days of Noah, that lived by himself. And he sinned not; for he had no father or mother: but his wife was righteous before God and her children were righteous before God.\n",
      "\n",
      "And Noah's sons were of the sons of Shem; and they all lived in the land of Shinar. And the men of Ur became gods to their gods, and worshipped them. And they worshiped Molech, and made an idol thereof, and served Molech, and did strange work after the manner of those who serve Moloch. And they worshiped Molech, and did strange work after the manner of those who serve Moloch, even unto this day. And the whole congregation of them all went out from Ur of the Chaldeans, and built a city called Sodom, and a tower named Bethel, which is in the wilderness of Gilead. And it fell upon the people of Sodom, and upon the inhabitants of the whole city, great confusion arose, and many died. And it came to pass, as soon as the sun was risen over the cities, that the ark of the Lord was brought into the city, and the ark of the Lord descended into the midst of the city, and stood still on the top of the mountain of Olabah. And the ark of the Lord rested upon Mount Gerizim, which is the highest place in the land of Canaan, and was set upon the top of the mountain. And the ark of the Lord rested there, and the ark of the Lord was carried away with him, and the ark of the Lord went up with him, until he reached the top of the mountain. And there were also seven other men, whom the Lord had taken away, and who remained in the wilderness, to make them gods to their gods, and to serve Molech. And the Lord said unto Abram, Arise and go back unto thy fathers: for I have heard that thou hast done well, and my servant Isaac shall bless thee, and thy offspring shall be blessed. But the seven others departed from me, and came not unto Abram. And Abram was very sorrowful because of them. And they came to Abraham again, and said, We will go with thee to the mountains of Ephron, that we may give unto our servants Isaac a portion of the land. And Abraham sent two of his sons, and\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(base_model, \"./christAIn\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def ask(question):\n",
    "    inputs = tokenizer(f\"### Question:\\n{question}\\n\\n### Answer (in KJV style):\", return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=500, temperature=0.7)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(ask(\"What does the Bible say about love?\"))\n",
    "print(ask(\"What was the original sin?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f685c372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question:\n",
      "What does the Bible say about love?\n",
      "\n",
      "### Answer (in KJV style):\n",
      "\n",
      "And the Lord said unto Moses, I have heard thy voice in the wilderness: and behold, I will send thee a cloud to overshadow thee from afar off; and thou shalt see a fire upon the face of the land. And when the fire shall come down, and cover all the land of Egypt, so that there shall be no more water for thee to drink, then shall the people go out of the land of Egypt. And they shall eat the fat of the flock and of the herd, and the flesh of the ox, and the fat of the sheep, and the fat of the venison, and the fat of the fowl of the field, and the fat of the fat of the fowl of the sea, and the fat of the fat of the fowl of the mountains, and the fat of the fat of the fowl of the nest, and the fat of the fat of the fowl of the cave, and the fat of the fat of the fowl of the orchard, and the fat of the fat of the fowl of the vineyard, and the fat of the fat of the fowl of the olive tree, and the fat of the fat of the fowl of the fig-tree, and the fat of the fat of the fowl of the cypress tree, and the fat of the fat of the fowl of the tamarisk tree, and the fat of the fat of the fowl of the hawthorn tree, and the fat of the fat of the fowl of the pomegranate tree, and the fat of the fat of the fowl of the olive tree, and the fat of the fat of the fowl of the grape tree, and the fat of the fat of the fowl of the fig-tree, and the fat of the fat of the fowl of the cypress tree, and the fat of the fat of the fowl of the tamarisk tree, and the fat of the fat of the fowl of the hawthorn tree, and the fat of the fat of the fowl of the pomegranate tree, and the fat of the fat of the fowl of the olive tree, and the fat of the fat of the fowl of the grape tree, and the fat of the fat of the fowl of the fig-tree, and the fat of the fat of the fowl of the cypress tree, and the fat of the fat\n",
      "### Question:\n",
      "What was the original sin?\n",
      "\n",
      "### Answer (in KJV style):\n",
      "\n",
      "**1. The first Adam, in his own image, was formed; and he knew no sin:**\n",
      "\n",
      "**2. And the second Adam, also in his own image, was formed; and he knew no sin:**\n",
      "\n",
      "**3. But when the evening came, and it was day, the eyes of all flesh were turned from the work of God unto the work of man:**\n",
      "\n",
      "**4. For the eye of the Lord saw not anything that was done by man, but did see what was done by God:**\n",
      "\n",
      "**5. And there was a great difference between the two, because they had both sinned after their own hearts.**\n",
      "\n",
      "**6. And the Lord said unto them, Behold, I have made you to be gods for yourselves, and have put you to be lords over your wives and children, and over all the beasts of the field and of the sea, and over all the birds of the heaven and on earth: and now shall you know that I am the Lord, who maketh all things.**\n",
      "\n",
      "**7. And the Lord said unto them, Because you have sinned against me, and have eaten of the tree of the knowledge of good and evil, therefore I will destroy you from off the face of the earth: and ye shall all be exterminated.**\n",
      "\n",
      "**8. And the Lord said unto them, If you eat of the tree of the knowledge of good and evil, then you shall die: and if you eat of the tree of life, then you shall live.**\n",
      "\n",
      "**9. And the Lord said unto them, Behold, I have given you another commandment, which is this: You shall not eat of any tree of the tree of the knowledge of good and evil, lest you die.**\n",
      "\n",
      "**10. And the Lord said unto them, Behold, I have given you another commandment, which is this: Thou shalt not eat of any tree of the tree of the knowledge of good and evil, lest thou die.**\n",
      "\n",
      "**11. And the Lord said unto them, Behold, I have given you another commandment, which is this: Thou shalt not eat of any tree of the tree of the knowledge of good and evil, lest thou die.**\n",
      "\n",
      "**12. And the Lord said unto them, Behold, I have given you another commandment, which is this: Thou shalt not eat of any tree\n"
     ]
    }
   ],
   "source": [
    "def ask_kjv(question, max_tokens=500):\n",
    "    inputs = tokenizer(\n",
    "        f\"### Question:\\n{question}\\n\\n### Answer (in KJV style):\",\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        temperature=0.5,\n",
    "        do_sample=True,\n",
    "        top_p=0.5\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(ask_kjv(\"What does the Bible say about love?\"))\n",
    "print(ask_kjv(\"What was the original sin?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a52958f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Question:\n",
      "Are there dinosaurs?\n",
      "\n",
      "### Answer (in KJV style):\n",
      "\n",
      "**Thou art a great and mighty God, O Jehovah; the earth is thy dwelling place. Thou art a king in the midst of the earth: thou art a prince upon the face of the waters. The earth is full of thy works, and all that thou hast done. Thy footprints are upon every mountain, and on every hill, and on every plain, and on every river, and on every good land. Thou art a great and powerful God, O Jehovah, and art a mighty warrior. Thou art a king upon the face of the earth, and art a prince upon the face of the waters. Thou art a great and mighty God, O Jehovah, and art a mighty warrior. Thou art a king upon the face of the earth, and art a prince upon the face of the waters. Thou art a great and mighty God, O Jehovah, and art a mighty warrior. Thou art a king upon the face of the earth, and art a prince upon the face of the waters. Thou art a great and mighty God, O Jehovah, and art a mighty warrior. Thou art a king upon the face of the earth, and art a prince upon the face of the waters. Thou art a great and mighty God, O Jehovah, and art a mighty warrior. Thou art a king upon the face of the earth, and art a prince upon the face of the waters. Thou art a great and mighty God, O Jehovah, and art a mighty warrior. Thou\n"
     ]
    }
   ],
   "source": [
    "print(ask_kjv(\"Are there dinosaurs?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed12b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model merged and saved!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "base_model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"  # Original base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# Load LoRA adapter\n",
    "lora_model = PeftModel.from_pretrained(model, \"./christAIn/\")\n",
    "\n",
    "# Merge and save\n",
    "merged_model = lora_model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"./christAIn-merged/\")\n",
    "tokenizer.save_pretrained(\"./christAIn-merged/\")\n",
    "\n",
    "print(\"Model merged and saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a48337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
